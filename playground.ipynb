{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4052fba4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d826fe",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b468575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a style for all plots\n",
    "plt.style.use(\"ggplot\")\n",
    "# Set the maximum number of columns to display for DataFrames\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# --- File Paths ---\n",
    "BASE_PATH = \"./dataset/\"\n",
    "\n",
    "TRAIN_PATH = f\"{BASE_PATH}train.csv\"\n",
    "LABELS_PATH = f\"{BASE_PATH}train_labels.csv\"\n",
    "PAIRS_PATH = f\"{BASE_PATH}target_pairs.csv\"\n",
    "TEST_PATH = f\"{BASE_PATH}test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c710c6b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 1: Analyze train.csv ---\n",
    "print(\"--- Loading train.csv ---\")\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_train['date_id'] = pd.to_datetime(df_train['date_id'])\n",
    "print(f\"Train data shape: {df_train.shape}\")\n",
    "\n",
    "\n",
    "# --- Visualization of Time Series ---\n",
    "print(\"\\n--- Generating Time Series Visualizations ---\")\n",
    "all_columns = df_train.columns.tolist()\n",
    "\n",
    "# Find the first column for each source. This is robust.\n",
    "lme_col_to_plot = [col for col in all_columns if col.startswith('LME_')][0]\n",
    "jpx_col_to_plot = [col for col in all_columns if col.startswith('JPX_')][0]\n",
    "us_stock_col_to_plot = [col for col in all_columns if col.startswith('US_STOCK_')][0] if any(col.startswith('US_STOCK_') for col in all_columns) else None\n",
    "fx_col_to_plot = [col for col in all_columns if col.startswith('FX_')][0] if any(col.startswith('FX_') for col in all_columns) else None\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "fig.suptitle('Sample Time Series from Different Markets', fontsize=16)\n",
    "\n",
    "def plot_series(ax, column_name, title):\n",
    "    if column_name:\n",
    "        sns.lineplot(data=df_train, x='date_id', y=column_name, ax=ax)\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        ax.set_title(f'{title}\\n(No data found)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "plot_series(axes[0, 0], lme_col_to_plot, f'LME: {lme_col_to_plot}')\n",
    "plot_series(axes[0, 1], jpx_col_to_plot, f'JPX: {jpx_col_to_plot}')\n",
    "plot_series(axes[1, 0], us_stock_col_to_plot, f'US Stock: {us_stock_col_to_plot or \"N/A\"}')\n",
    "plot_series(axes[1, 1], fx_col_to_plot, f'FX: {fx_col_to_plot or \"N/A\"}')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Section 2: Analyze train_labels.csv ---\n",
    "print(\"\\n--- Loading train_labels.csv ---\")\n",
    "df_labels = pd.read_csv(LABELS_PATH)\n",
    "df_labels['date_id'] = pd.to_datetime(df_labels['date_id'])\n",
    "print(f\"Train labels shape: {df_labels.shape}\")\n",
    "\n",
    "print(\"\\n--- Generating Target Distribution Visualizations ---\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "fig.suptitle('Distribution of Sample Target Values', fontsize=16)\n",
    "\n",
    "sns.histplot(df_labels['target_10'], kde=True, ax=axes[0, 0], bins=50).set_title('Distribution of target_10')\n",
    "sns.histplot(df_labels['target_110'], kde=True, ax=axes[0, 1], bins=50).set_title('Distribution of target_110')\n",
    "sns.histplot(df_labels['target_210'], kde=True, ax=axes[1, 0], bins=50).set_title('Distribution of target_210')\n",
    "sns.histplot(df_labels['target_410'], kde=True, ax=axes[1, 1], bins=50).set_title('Distribution of target_410')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Section 3: Analyze target_pairs.csv ---\n",
    "print(\"\\n--- Loading target_pairs.csv ---\")\n",
    "df_pairs = pd.read_csv(PAIRS_PATH)\n",
    "print(f\"Target pairs shape: {df_pairs.shape}\")\n",
    "\n",
    "print(\"\\n--- Analyzing Target Recipes ---\")\n",
    "df_pairs['is_pair'] = df_pairs['pair'].apply(lambda x: '-' in str(x))\n",
    "pair_counts = df_pairs['is_pair'].value_counts()\n",
    "\n",
    "print(f\"Number of single-asset targets: {pair_counts.get(False, 0)}\")\n",
    "print(f\"Number of paired-asset targets: {pair_counts.get(True, 0)}\")\n",
    "\n",
    "lag_counts = df_pairs['lag'].value_counts()\n",
    "print(\"\\n--- Distribution of Time Lags ---\")\n",
    "print(lag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Handling Missing Values ---\n",
    "\n",
    "# We'll use forward-fill to handle missing data.\n",
    "# This propagates the last valid observation forward.\n",
    "# We will fill the remaining NaN's with 0\n",
    "df_train_processed = df_train.fillna(method='ffill').fillna(0)\n",
    "\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(\"Missing values after forward-filling and back-filling:\")\n",
    "print(df_train_processed.isnull().sum().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Creating Lag Features ---\n",
    "\n",
    "# Let's create lag features for a few selected columns to demonstrate.\n",
    "# We'll pick the same columns we visualized earlier.\n",
    "cols_to_lag = [\n",
    "    'LME_AH_Close', \n",
    "    'JPX_Gold_Standard_Futures_Close',\n",
    "    'US_Stock_VTV_adj_close',\n",
    "    'FX_USDJPY'\n",
    "]\n",
    "\n",
    "# We are creating 1, 2, and 3-day lags\n",
    "for col in cols_to_lag:\n",
    "    for lag in range(1, 4):\n",
    "        # The new column name will be like 'LME_AH_Close_lag1'\n",
    "        df_train_processed[f'{col}_lag{lag}'] = df_train_processed[col].shift(lag)\n",
    "\n",
    "# Display the new lag features for the first few rows\n",
    "# Note: The first few rows will have NaN for lag features, which is expected.\n",
    "print(\"DataFrame with new Lag Features (showing tail):\")\n",
    "display(df_train_processed.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating Rolling Window Features ---\n",
    "\n",
    "# We'll use the same columns for consistency\n",
    "cols_to_roll = [\n",
    "    'LME_AH_Close', \n",
    "    'JPX_Gold_Standard_Futures_Close',\n",
    "    'US_Stock_VTV_adj_close',\n",
    "    'FX_USDJPY'\n",
    "]\n",
    "\n",
    "# Let's define a window size, e.g., 5 days and 20 days\n",
    "window_sizes = [5, 20]\n",
    "\n",
    "for col in cols_to_roll:\n",
    "    for window in window_sizes:\n",
    "        # Rolling Mean\n",
    "        df_train_processed[f'{col}_roll_mean_{window}'] = df_train_processed[col].rolling(window=window).mean()\n",
    "        # Rolling Standard Deviation (Volatility)\n",
    "        df_train_processed[f'{col}_roll_std_{window}'] = df_train_processed[col].rolling(window=window).std()\n",
    "\n",
    "# Display the new rolling features\n",
    "print(\"DataFrame with new Rolling Window Features (showing tail):\")\n",
    "display(df_train_processed.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09029b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Merging Features with Labels ---\n",
    "\n",
    "# Merge the processed training data with the labels on 'date_id'\n",
    "final_df = pd.merge(df_train_processed, df_labels, on='date_id', how='inner')\n",
    "\n",
    "# After creating lagged features, the first few rows will have NaNs.\n",
    "# It's best practice to drop these rows as they cannot be used for training.\n",
    "final_df = final_df.dropna()\n",
    "\n",
    "print(f\"Shape of the final, merged DataFrame: {final_df.shape}\")\n",
    "print(\"Final DataFrame ready for model training (showing head):\")\n",
    "display(final_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Rolling Mean ---\n",
    "\n",
    "# We will use the final_df which has been processed and merged\n",
    "# Let's focus on one asset to see the effect clearly\n",
    "asset_to_plot = 'LME_AH_Close'\n",
    "window_sizes = [5, 20] # The windows we created earlier\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "# Plot the original closing price\n",
    "plt.plot(final_df['date_id'], final_df[asset_to_plot], label='Original Price', alpha=0.6)\n",
    "\n",
    "# Plot the rolling means\n",
    "for window in window_sizes:\n",
    "    plt.plot(final_df['date_id'], final_df[f'{asset_to_plot}_roll_mean_{window}'], label=f'{window}-Day Rolling Mean')\n",
    "\n",
    "plt.title(f'Original Price vs. Rolling Means for {asset_to_plot}', fontsize=16)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba17856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Visualize Rolling Volatility ---\n",
    "\n",
    "asset_to_plot = 'JPX_Gold_Standard_Futures_Close'\n",
    "window_size = 20 # A common window for monthly volatility\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(final_df['date_id'], final_df[f'{asset_to_plot}_roll_std_{window_size}'])\n",
    "plt.title(f'{window_size}-Day Rolling Volatility for {asset_to_plot}', fontsize=16)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Standard Deviation (Volatility)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435beddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate and Visualize Feature Correlation ---\n",
    "\n",
    "# Select the features we created for a specific asset\n",
    "features_to_correlate = [\n",
    "    'LME_AH_Close_lag1', 'LME_AH_Close_lag2', 'LME_AH_Close_lag3',\n",
    "    'LME_AH_Close_roll_mean_5', 'LME_AH_Close_roll_std_5',\n",
    "    'LME_AH_Close_roll_mean_20', 'LME_AH_Close_roll_std_20'\n",
    "]\n",
    "\n",
    "# Select a target to correlate against\n",
    "target_col = 'target_0'\n",
    "\n",
    "# Calculate the correlation of each feature with the target\n",
    "correlations = final_df[features_to_correlate + [target_col]].corr()[target_col].drop(target_col)\n",
    "\n",
    "# Plot the correlations\n",
    "plt.figure(figsize=(12, 7))\n",
    "correlations.sort_values().plot(kind='barh', color='skyblue')\n",
    "plt.title(f'Correlation of Engineered Features with {target_col}', fontsize=16)\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(axis='x', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Visualize Feature Distributions ---\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Distribution of Engineered Features', fontsize=16)\n",
    "\n",
    "# Plot distribution of a lag feature\n",
    "sns.histplot(final_df['LME_AH_Close_lag1'], kde=True, ax=axes[0, 0], bins=40)\n",
    "axes[0, 0].set_title('Distribution of LME_AH_Close_lag1')\n",
    "\n",
    "# Plot distribution of a rolling mean feature\n",
    "sns.histplot(final_df['LME_AH_Close_roll_mean_20'], kde=True, ax=axes[0, 1], bins=40)\n",
    "axes[0, 1].set_title('Distribution of LME_AH_Close_roll_mean_20')\n",
    "\n",
    "# Plot distribution of a rolling std feature\n",
    "sns.histplot(final_df['JPX_Gold_Standard_Futures_Close_roll_std_20'], kde=True, ax=axes[1, 0], bins=40)\n",
    "axes[1, 0].set_title('Distribution of JPX Gold Volatility')\n",
    "\n",
    "# Plot distribution of another rolling std feature\n",
    "sns.histplot(final_df['US_Stock_VTV_adj_close_roll_std_20'], kde=True, ax=axes[1, 1], bins=40)\n",
    "axes[1, 1].set_title('Distribution of US Stock VTV Volatility')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113694d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# First, re-create final_df with the corrected index logic\n",
    "# ==================================================================\n",
    "import os \n",
    "# Merging Features with Labels (Corrected)\n",
    "final_df = pd.merge(df_train_processed, df_labels, on='date_id', how='inner')\n",
    "\n",
    "# After creating lagged features, the first few rows will have NaNs.\n",
    "final_df = final_df.dropna()\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# Reset the index to be a clean 0, 1, 2, ... sequence\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Shape of the final, merged DataFrame: {final_df.shape}\")\n",
    "print(\"Final DataFrame ready for model training (showing head):\")\n",
    "display(final_df.head())\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Now, run the full training pipeline with the corrected final_df\n",
    "# ==================================================================\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import spearmanr\n",
    "import gc\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "class CFG:\n",
    "    N_SPLITS = 5\n",
    "    N_TOP_FEATURES = 200\n",
    "    N_TARGETS_TO_TRAIN = 423\n",
    "    TARGET_COLS = [f'target_{i}' for i in range(423)]\n",
    "    \n",
    "    LGB_PARAMS = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mae',\n",
    "        'n_estimators': 5000, # <-- INCREASE THIS VALUE\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 1,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'boosting_type': 'gbdt',\n",
    "    }\n",
    "\n",
    "# --- 1. Custom Evaluation Metric ---\n",
    "def sharpe_ratio_metric(preds, labels):\n",
    "    daily_correlations = []\n",
    "    for i in range(len(preds)):\n",
    "        if np.std(preds[i]) > 0 and np.std(labels[i]) > 0:\n",
    "            daily_correlations.append(spearmanr(preds[i], labels[i]).correlation)\n",
    "        else:\n",
    "            daily_correlations.append(0)\n",
    "    \n",
    "    daily_correlations = np.array(daily_correlations)\n",
    "    mean_corr = np.nanmean(daily_correlations)\n",
    "    std_corr = np.nanstd(daily_correlations)\n",
    "    \n",
    "    if std_corr > 0:\n",
    "        return mean_corr / std_corr\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# --- 2. Feature Selection ---\n",
    "print(\"--- Starting Feature Selection ---\")\n",
    "all_features = [col for col in final_df.columns if col not in ['date_id'] + CFG.TARGET_COLS]\n",
    "X = final_df[all_features]\n",
    "y = final_df[CFG.TARGET_COLS]\n",
    "y_proxy = y.mean(axis=1)\n",
    "\n",
    "temp_model = lgb.LGBMRegressor(**CFG.LGB_PARAMS)\n",
    "temp_model.fit(X, y_proxy)\n",
    "\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': temp_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_features = feature_importances['feature'].head(CFG.N_TOP_FEATURES).tolist()\n",
    "print(f\"Selected {len(top_features)} features.\")\n",
    "X = X[top_features]\n",
    "\n",
    "print(f\"\\n--- Starting Full CV Pipeline for {CFG.N_TARGETS_TO_TRAIN} Targets ---\")\n",
    "\n",
    "oof_df = pd.DataFrame(index=X.index, columns=CFG.TARGET_COLS)\n",
    "y_oof = y.copy()\n",
    "tscv = TimeSeriesSplit(n_splits=CFG.N_SPLITS)\n",
    "targets_to_run = CFG.TARGET_COLS[:CFG.N_TARGETS_TO_TRAIN]\n",
    "\n",
    "for i, target_name in enumerate(targets_to_run):\n",
    "    print(f\"\\n--- Training for {target_name} ({i+1}/{len(targets_to_run)}) ---\")\n",
    "    y_single_target = y[target_name]\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y_single_target.iloc[train_index], y_single_target.iloc[val_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**CFG.LGB_PARAMS)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        \n",
    "        # This line is where the predictions are generated and stored\n",
    "        oof_df.loc[val_index, target_name] = model.predict(X_val)\n",
    "        \n",
    "        del X_train, X_val, y_train, y_val, model\n",
    "        gc.collect()\n",
    "\n",
    "# ==================================================================\n",
    "# PART 4: FINAL EVALUATION\n",
    "# Now that oof_df is filled, we can calculate the score.\n",
    "# ==================================================================\n",
    "print(\"\\n--- Calculating Final CV Score ---\")\n",
    "oof_df.dropna(how='all', inplace=True)\n",
    "y_oof = y_oof.loc[oof_df.index]\n",
    "targets_trained = [col for col in targets_to_run if col in oof_df.columns]\n",
    "oof_df = oof_df[targets_trained]\n",
    "y_oof = y_oof[targets_trained]\n",
    "\n",
    "oof_preds_np = oof_df.to_numpy()\n",
    "y_oof_np = y_oof.to_numpy()\n",
    "\n",
    "final_sharpe_score = sharpe_ratio_metric(oof_preds_np, y_oof_np)\n",
    "\n",
    "print(f\"\\n==========================================================\")\n",
    "print(f\"Final Cross-Validated Sharpe Ratio: {final_sharpe_score:.4f}\")\n",
    "print(f\"==========================================================\")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# PART 5: TRAIN FINAL MODELS AND SAVE ARTIFACTS FOR SUBMISSION\n",
    "# This is the final step, done after evaluation is complete.\n",
    "# ==================================================================\n",
    "print(\"\\n--- Training final models on all data and saving artifacts ---\")\n",
    "\n",
    "# --- 5a. Save the list of top features ---\n",
    "FEATURES_PATH = '/kaggle/working/top_200_features.txt'\n",
    "with open(FEATURES_PATH, 'w') as f:\n",
    "    for feature in top_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(f\"Successfully saved feature list to: {FEATURES_PATH}\")\n",
    "\n",
    "# --- 5b. Train and save one final model per target ---\n",
    "MODELS_DIR = '/kaggle/working/models/'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "for i, target_name in enumerate(CFG.TARGET_COLS):\n",
    "    print(f\"  -> Training final model for {target_name} ({i+1}/{len(CFG.TARGET_COLS)})\")\n",
    "    y_single_target = y[target_name]\n",
    "    final_model = lgb.LGBMRegressor(**CFG.LGB_PARAMS)\n",
    "    final_model.fit(X, y_single_target)\n",
    "    model_path = os.path.join(MODELS_DIR, f'model_{target_name}.txt')\n",
    "    final_model.booster_.save_model(model_path)\n",
    "\n",
    "print(f\"\\nSuccessfully trained and saved {len(CFG.TARGET_COLS)} models to the '{MODELS_DIR}' directory.\")\n",
    "print(\"\\nYour artifacts are now ready for the submission/inference notebook!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ==================================================================\n",
    "# PART 1: LOAD ARTIFACTS AND INITIALIZE GLOBAL STATE\n",
    "# This part runs only once when your notebook starts.\n",
    "# ==================================================================\n",
    "print(\"--- Loading models and features for submission ---\")\n",
    "\n",
    "MODELS_DIR = '/kaggle/input/trained-feature-model/models'\n",
    "FEATURES_PATH = '/kaggle/input/trained-feature-model/top_200_features.txt'\n",
    "\n",
    "# Load the list of top features we trained on\n",
    "with open(FEATURES_PATH, 'r') as f:\n",
    "    top_features = [line.strip() for line in f]\n",
    "\n",
    "# Load all 424 trained LightGBM models into memory\n",
    "loaded_models = {}\n",
    "for i in range(424):\n",
    "    target_name = f'target_{i}'\n",
    "    model_path = os.path.join(MODELS_DIR, f'model_{target_name}.txt')\n",
    "    if os.path.exists(model_path):\n",
    "        loaded_models[target_name] = lgb.Booster(model_file=model_path)\n",
    "print(f\"Loaded {len(loaded_models)} models.\")\n",
    "\n",
    "# Initialize a global history buffer with the tail of our training data\n",
    "# This is crucial for calculating rolling features on the first test day.\n",
    "# `final_df` should be the final training dataframe from your previous cells.\n",
    "history_df = final_df.tail(60).copy()\n",
    "ycols = [f'target_{i}' for i in range(424)]\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# PART 2: THE PREDICT FUNCTION\n",
    "# The Kaggle environment will call this function repeatedly.\n",
    "# ==================================================================\n",
    "\n",
    "def predict(\n",
    "    test: pd.DataFrame,\n",
    "    lag1: pd.DataFrame, \n",
    "    lag2: pd.DataFrame,\n",
    "    lag3: pd.DataFrame,\n",
    "    lag4: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predicts target values using our trained LightGBM models and feature engineering.\n",
    "    \"\"\"\n",
    "    global history_df, loaded_models, top_features, ycols\n",
    "\n",
    "    # --- 1. Data Conversion and Handling Empty Input ---\n",
    "    if hasattr(test, 'to_pandas'):\n",
    "        test = test.to_pandas()\n",
    "    \n",
    "    if len(test) == 0:\n",
    "        return pd.DataFrame(0, index=range(1), columns=ycols)\n",
    "\n",
    "    # --- 2. Feature Engineering with History Buffer ---\n",
    "    # Append new test data to our history\n",
    "    history_df = pd.concat([history_df, test], ignore_index=True)\n",
    "    latest_row_idx = history_df.index[-1]\n",
    "    \n",
    "    # Calculate features for the new row on-the-fly\n",
    "    for col in top_features:\n",
    "        if '_lag' in col:\n",
    "            original_col, lag_str = col.rsplit('_lag', 1)\n",
    "            lag_num = int(lag_str)\n",
    "            if original_col in history_df.columns:\n",
    "                history_df.loc[latest_row_idx, col] = history_df.loc[latest_row_idx - lag_num, original_col]\n",
    "        elif '_roll_' in col:\n",
    "            parts = col.split('_roll_')\n",
    "            original_col = parts[0]\n",
    "            agg_type, window_str = parts[1].split('_', 1)\n",
    "            window = int(window_str)\n",
    "            if original_col in history_df.columns:\n",
    "                window_data = history_df[original_col].tail(window)\n",
    "                if agg_type == 'mean':\n",
    "                    history_df.loc[latest_row_idx, col] = window_data.mean()\n",
    "                elif agg_type == 'std':\n",
    "                    history_df.loc[latest_row_idx, col] = window_data.std()\n",
    "    \n",
    "    # Get the final feature vector for the current day\n",
    "    current_features = history_df[top_features].iloc[-1].fillna(0)\n",
    "\n",
    "    # --- 3. Prediction with LightGBM Models ---\n",
    "    preds = {}\n",
    "    for i in range(424):\n",
    "        target_name = f'target_{i}'\n",
    "        model = loaded_models.get(target_name)\n",
    "        if model:\n",
    "            # Predict using the single feature vector\n",
    "            pred = model.predict(current_features.values.reshape(1, -1))[0]\n",
    "            preds[target_name] = pred\n",
    "        else:\n",
    "            preds[target_name] = 0.0 # Default if model is missing\n",
    "\n",
    "    # Create the final prediction DataFrame\n",
    "    preds_df = pd.DataFrame([preds], columns=ycols)\n",
    "    \n",
    "    return preds_df\n",
    "\n",
    "# ==================================================================\n",
    "# PART 3: API INITIALIZATION AND SERVER RUN\n",
    "# This part connects our function to the Kaggle environment.\n",
    "# ==================================================================\n",
    "# Add the directory containing the API script to the Python path\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/')\n",
    "\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "inference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))\n",
    "# Display the local test submission file\n",
    "# display(pd.read_parquet('/kaggle/working/submission.parquet'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
